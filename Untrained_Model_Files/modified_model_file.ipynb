{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b99f7d-ee2b-4bff-b34d-12c98c74dfad",
   "metadata": {},
   "source": [
    "**LSTM MODEL**\n",
    "1. Install and import dependencies\n",
    "2. Keypoints using MP Holistic\n",
    "3. Extract Keypoints\n",
    "4. Setup Folders for Collection\n",
    "5. Collect Keypoints Values for Training and Testing\n",
    "6. Preprocess Data and Create Labels and Features\n",
    "7. Build and Train LSTM Neural Network\n",
    "8. Make Predictions\n",
    "9. Save Weights\n",
    "10. Evaluation using Confusion Matrix and Accuracy\n",
    "11. Test in Real Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d77333e-d0ef-43c2-bee0-f4504680c5ac",
   "metadata": {},
   "source": [
    "# Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dafcdf14-dd09-49af-be3f-2f364b0dd831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (2.15.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\brianna\\appdata\\roaming\\python\\python311\\site-packages (0.10.14)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: tensorflowjs in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (4.21.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.1 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow) (2.15.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (4.25.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-intel==2.15.1->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: jax in c:\\users\\brianna\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.34)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\brianna\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.34)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\brianna\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from mediapipe) (0.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: flax>=0.7.2 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflowjs) (0.9.0)\n",
      "Requirement already satisfied: importlib_resources>=5.9.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflowjs) (6.4.5)\n",
      "Requirement already satisfied: tf-keras>=2.13.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflowjs) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-decision-forests>=1.5.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflowjs) (1.8.1)\n",
      "Requirement already satisfied: tensorflow-hub>=0.16.1 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflowjs) (0.16.1)\n",
      "Requirement already satisfied: msgpack in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from flax>=0.7.2->tensorflowjs) (1.1.0)\n",
      "Requirement already satisfied: optax in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from flax>=0.7.2->tensorflowjs) (0.2.3)\n",
      "Requirement already satisfied: orbax-checkpoint in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from flax>=0.7.2->tensorflowjs) (0.7.0)\n",
      "Requirement already satisfied: tensorstore in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from flax>=0.7.2->tensorflowjs) (0.1.66)\n",
      "Requirement already satisfied: rich>=11.1 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from flax>=0.7.2->tensorflowjs) (13.9.2)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from flax>=0.7.2->tensorflowjs) (6.0.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.2.2)\n",
      "Requirement already satisfied: wheel in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.44.0)\n",
      "Requirement already satisfied: wurlitzer in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (3.1.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.15.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.1->tensorflow) (2.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.1->tensorflow) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.1->tensorflow) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.1->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.1->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: chex>=0.1.86 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.87)\n",
      "Requirement already satisfied: etils[epy] in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from optax->flax>=0.7.2->tensorflowjs) (1.9.4)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
      "Requirement already satisfied: humanize in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (4.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2024.1)\n",
      "Requirement already satisfied: toolz>=0.9.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from chex>=0.1.86->optax->flax>=0.7.2->tensorflowjs) (1.0.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.1->tensorflow) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.1->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.1->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.1->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.1->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.1->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.1->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.1->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.1->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2024.9.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.20.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.1->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\brianna\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.1->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow opencv-python mediapipe scikit-learn matplotlib tensorflowjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402d7845-ef54-4f64-b6a5-03e9d2654c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Brianna\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadadbe2-263e-48c7-a494-eb71121a6cb1",
   "metadata": {},
   "source": [
    "# Keypoints using Media Pipes Holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54818ce-85bc-4f57-8574-4a441383c611",
   "metadata": {},
   "source": [
    "This cell is importing the holistic model and drawing utilities from the mediapipe library:\n",
    "\n",
    "- **`mp.solutions.holistic`**: This is responsible for detecting landmarks across the entire body, including the face, hands, and pose.\n",
    "\n",
    "- **`mp.solutions.drawing_utils`**: It draws lines, points, or other shapes over the detected landmarks to make them visible, helping you understand the positions of various body parts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a3c3ea-500b-4c9d-ac98-a27f2a4321f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86f019-dd39-46b6-a189-4cc9fcdd1ca3",
   "metadata": {},
   "source": [
    "This cell creates a function that processes a image using a specified media pipes model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b0c9bd7-1c87-4d6d-be2f-6fa1da062e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR-CONVERSION BGR-to-RGB\n",
    "    image.flags.writeable = False                  # Convert image to not-writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Convert image to writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR-COVERSION RGB-to-BGR\n",
    "    return image, results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccbc40c-eed8-4edb-bc26-2deecf32c3af",
   "metadata": {},
   "source": [
    "This cell draws the landmarks using media pipes for the face, pose (shoulder/arms/neckline) and hands(left and right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e57088-8d3d-43b6-9cf5-0b5f8b675d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c68e19-6ea0-4336-a489-6d7ea2153df5",
   "metadata": {},
   "source": [
    "This shows the media pipes lines on the image when they access the camera "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b3c877b-9576-4c3f-a6a9-43e9605b5ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce246e3-2e35-48e4-a822-afbc77e001f4",
   "metadata": {},
   "source": [
    "## This is a test cell\n",
    "# Do not run this cell if you want to save time\n",
    "Running this cell just tests webcam access, and applies all the mediapipe landmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0149efc8-db73-4bb8-ada2-9772ec98b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "# # Set mediapipe model \n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#     while cap.isOpened():\n",
    "\n",
    "#         # Read feed\n",
    "#         ret, frame = cap.read()\n",
    "\n",
    "#         # Make detections\n",
    "#         image, results = mediapipe_detection(frame, holistic)\n",
    "#         print(results)\n",
    "        \n",
    "#         # Draw landmarks\n",
    "#         draw_styled_landmarks(image, results)\n",
    "\n",
    "#         # Show to screen\n",
    "#         cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "#         # Break gracefully\n",
    "#         if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#             break\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34ffa85-04b1-4836-8050-84961ffa12e0",
   "metadata": {},
   "source": [
    "If you ran the cell above then this cell shows you the last frame that was taken and applies the mediapipes functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e45e8f62-2d16-40c9-8e62-c068bbf8ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_landmarks(frame, results)\n",
    "# plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e09683-7cd9-42a8-8676-d48c2ff830f2",
   "metadata": {},
   "source": [
    "### `extract_keypoints` Function Overview\n",
    "\n",
    "The `extract_keypoints` function extracts the coordinates of detected landmarks from the Mediapipe model's results, which represent various features of the body, face, and hands. \n",
    "\n",
    "#### Key Steps:\n",
    "\n",
    "1. **Input**: Takes `results` from the Mediapipe model containing landmark data for the current frame.\n",
    "  \n",
    "2. **Coordinate Extraction**:\n",
    "   - **Pose Landmarks**: Extracts x, y, z coordinates and visibility for body landmarks.\n",
    "   - **Face Landmarks**: Extracts x, y, z coordinates for facial landmarks.\n",
    "   - **Left Hand Landmarks**: Extracts x, y, z coordinates for left hand landmarks.\n",
    "   - **Right Hand Landmarks**: Extracts x, y, z coordinates for right hand landmarks.\n",
    "\n",
    "3. **Flattening**: Flattens these coordinates into one-dimensional arrays.\n",
    "\n",
    "4. **Handling Missing Landmarks**: Returns a zero array of appropriate size if landmarks are not detected.\n",
    "\n",
    "5. **Output**: Concatenates all extracted arrays into a single NumPy array for convenient processing.\n",
    "\n",
    "### Summary\n",
    "\n",
    "This function organizes the landmark coordinates into a structured format, making it suitable for further analysis, such as gesture recognition or pose estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc8b8276-c0bc-4629-8143-66eaf46196bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db42be77-502e-43b2-9db9-552f7d365a59",
   "metadata": {},
   "source": [
    "# Here we are setting up folders for data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42e7d85-e36c-4509-8b38-3952db710ef5",
   "metadata": {},
   "source": [
    "To create another model to define different words change the labels in this array ```python actions = np.array([...]) ```\n",
    "Ensure that you have a folder that you want to append the data to, this cell does not create a folder ```python DATA_PATH = os.path.join('MP_Data') ``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be8770e8-bce3-4780-ae36-9a734040d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Path for exported data, numpy arrays\n",
    "# DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# # Actions that we try to detect\n",
    "# actions = np.array(['Hello', 'Goodbye', 'Please'])\n",
    "\n",
    "# # Thirty videos worth of data\n",
    "# no_sequences = 30\n",
    "\n",
    "# # Videos are going to be 30 frames in length\n",
    "# sequence_length = 30\n",
    "\n",
    "# # Folder start\n",
    "# start_folder = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2066ee-f6aa-4978-92db-6244bf1428fe",
   "metadata": {},
   "source": [
    "# Fill in the code cell below\n",
    "The cell above is an outline the cell bellow just contains missing information that you can fill keep the cell above and just use the one below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a44d6bd5-310e-48b7-b484-1559e027e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') # fill in the name of the folder where you'd like to save your frames takes as numpy arrays\n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['Hello', 'Bye', 'Please', 'Yes', 'No']) # fill in the names of the gestures you are trying to detect/define frames for\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "start_folder = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc79a41-71c1-4aa0-a2a2-85091979afdf",
   "metadata": {},
   "source": [
    "# Fill in the code cell\n",
    "The code cell below starts to create the subfolders to hold the numpy arrays that represent each frame that you take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "013242c3-49d8-4c3c-8f08-de2fb141005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tHis cell creates the folder and sets up the sub folders \n",
    "\n",
    "signs = ['Hello', 'Bye', 'Please', 'Yes', 'No']\n",
    "\n",
    "parent_folder = 'MP_Data'\n",
    "\n",
    "if not os.path.exists(parent_folder):\n",
    "    os.mkdir(parent_folder)\n",
    "    for sign in signs:\n",
    "        sign_folder = os.path.join(parent_folder, sign)\n",
    "        os.mkdir(sign_folder)\n",
    "        for i in range(30):                                     #change the number 30 to the number of frames that you are taking\n",
    "            subfolder = os.path.join(sign_folder, str(i))\n",
    "            os.mkdir(subfolder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fc2d78-4f88-4fd4-863f-d23decc316fa",
   "metadata": {},
   "source": [
    "# Access the camera and start taking the frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c16a9b1-59dd-4687-aa19-4da50bff5d59",
   "metadata": {},
   "source": [
    "The code cell below has a waiting period before defining the next word in addition to have a pause before taking the next frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b97c65f-fd9d-4005-8dc1-0b61fb754a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time  # Ensure to import the time module\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# # Set mediapipe model \n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "#     # NEW LOOP\n",
    "#     # Loop through actions\n",
    "#     for action in actions:\n",
    "#         # Loop through sequences aka videos\n",
    "#         for sequence in range(no_sequences):\n",
    "#             # Loop through video length aka sequence length\n",
    "#             for frame_num in range(sequence_length):\n",
    "\n",
    "#                 # NEW: Add a wait time before reading the next frame\n",
    "#                 time.sleep(3)  ######################################## change 2 to the number of time you want between taking frames\n",
    "\n",
    "#                 # Read feed\n",
    "#                 ret, frame = cap.read()\n",
    "\n",
    "#                 # Make detections\n",
    "#                 image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "#                 # Draw landmarks\n",
    "#                 draw_styled_landmarks(image, results)\n",
    "\n",
    "#                 # NEW Apply wait logic\n",
    "#                 if frame_num == 0: \n",
    "#                     cv2.putText(image, 'STARTING COLLECTION', (120, 200), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "#                     cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15, 12), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                     # Show to screen\n",
    "#                     cv2.imshow('OpenCV Feed', image)\n",
    "#                     cv2.waitKey(500)\n",
    "#                 else: \n",
    "#                     cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15, 12), \n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                     # Show to screen\n",
    "#                     cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "#                 # NEW Export keypoints\n",
    "#                 keypoints = extract_keypoints(results)\n",
    "#                 npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "#                 np.save(npy_path, keypoints)\n",
    "\n",
    "#                 # Break gracefully\n",
    "#                 if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#                     break\n",
    "\n",
    "#             # Add a wait time after finishing all sequences for the current action\n",
    "#             if sequence == no_sequences - 1:  # Only wait after finishing all sequences for the current action\n",
    "#                 cv2.putText(image, 'Collection complete for {}. Prepare for next action.'.format(action), (15, 12), \n",
    "#                             cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "#                 cv2.imshow('OpenCV Feed', image)\n",
    "#                 cv2.waitKey(2000)  # Wait for 2 seconds before moving to the next action\n",
    "\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373bc6d5-f778-4c46-a58e-55e9e3d2d8e0",
   "metadata": {},
   "source": [
    "# The code cell below is for taking frames efficiently, one after the other and gesture after gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ee5f5de-0086-4a15-9537-37d8a9526961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brianna\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'MP_Data\\\\Hello\\\\20\\\\0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m extract_keypoints(results)\n\u001b[0;32m     39\u001b[0m npy_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_PATH, action, \u001b[38;5;28mstr\u001b[39m(sequence), \u001b[38;5;28mstr\u001b[39m(frame_num))\n\u001b[1;32m---> 40\u001b[0m np\u001b[38;5;241m.\u001b[39msave(npy_path, keypoints)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Break gracefully\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\numpy\\lib\\npyio.py:542\u001b[0m, in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    541\u001b[0m         file \u001b[38;5;241m=\u001b[39m file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 542\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[0;32m    545\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MP_Data\\\\Hello\\\\20\\\\0.npy'"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(500)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80a1fd35-0986-4867-95c8-2f69c5ed0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4de285-042c-4d0c-a199-92c58ea2d703",
   "metadata": {},
   "source": [
    "# Preprocess Data and Create Labels and Features\n",
    "\n",
    "This is where they start using Sk-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bae7ec7-dca4-496a-9922-4912574254a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df700a1-2a0b-434c-a5bc-9698ac346ee3",
   "metadata": {},
   "source": [
    "This cell creates a dictionary called label_map that maps action labels to their corresponding numerical indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "625d7526-695a-44c1-b5ef-4e4f53445a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ecb0b-13bb-4806-823f-329b080c2513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebcbf3cd-096e-40f2-923d-0f957ee741fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "241bf6dc-773d-420d-91df-bf3b5a122604",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40a32126-f87d-4548-b151-95be34e883b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b425340f-c717-43a7-ba55-d3ffef415dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec85c7-0b0f-404c-916a-aafceba807ec",
   "metadata": {},
   "source": [
    "# Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "762a01c1-3075-47bc-bcb7-37e93514f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6388a95a-0109-40e7-bed0-fd123ff6f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "#While training access tensorboard\n",
    "#tensorboard --logdir=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45ee4a2e-440a-4722-b170-def563d66429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Brianna\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4669294-1d5f-4d0f-a392-5f08b3eb8416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Brianna\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7810c694-bce7-445d-93ac-7472580a48b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Brianna\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Brianna\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Brianna\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Brianna\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Brianna\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Brianna\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 30, 1662), found shape=(None, 20, 1662)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#this was the original line of code but it takes very long to run \u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[tb_callback])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filehewyi_bv.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Brianna\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Brianna\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Brianna\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Brianna\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Brianna\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Brianna\\anaconda3\\envs\\Capstone\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 30, 1662), found shape=(None, 20, 1662)\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "#this was the original line of code but it takes very long to run \n",
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])\n",
    "\n",
    "\n",
    "# this see's if it converges early to try and save run time\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# model.fit(X_train, y_train, epochs=2000, validation_split=0.2, callbacks=[tb_callback, early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2190c03-a4c3-49d2-b491-f25d6b182edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f68c4-77ec-476b-99a2-97bad0bc8e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "from PIL import ImageFont\n",
    "font = ImageFont.truetype(\"arial.ttf\", 12)\n",
    "visualkeras.layered_view(model, legend=True, font=font,spacing=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd2c41-8b9a-49c4-8bb8-40de708889e0",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c940bc-c69d-4c0b-8ace-ffbe3450a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0552f41-00c7-4d01-9d4e-f416ef585848",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b51580d-8b1c-4892-b4de-3cccf2e16018",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc02865a-0557-42ee-824e-88d3278f504b",
   "metadata": {},
   "source": [
    "# Save Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ce6f1d-4625-404e-9860-9e3f296427ce",
   "metadata": {},
   "source": [
    "Remember to save the model and the model weights for when you want to deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be413f95-09d6-4b7a-ad8a-15f4b2113245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('./model.h5')\n",
    "# model.save_weights('./model_weights.h5');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be7fea-b754-48ce-a54a-650734cc54d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model # this deletes the model and free's up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dbd75f-9c4e-4d14-830b-f2369f8384e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0782bef0-a5b1-4d32-a4a9-ad74881e7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a0ed1-68f3-4f36-8ffb-26c97a5d7753",
   "metadata": {},
   "source": [
    "# Evaluation using Confusion Matrix\n",
    "You can skip this to speed up getting to live translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb084c-e5a2-4624-bb92-f27c1b149f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "yhat = model.predict(X_test)\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()\n",
    "multilabel_confusion_matrix(ytrue, yhat)\n",
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63adb9fd-a6d4-4e2b-9d85-54302a55752c",
   "metadata": {},
   "source": [
    "# Real time testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5fc19d-4110-46fc-b0e4-6c74e1eff72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ce620-0c5b-4d78-a76b-4194028a74d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf6012-323a-4a23-bfdf-b5fb22c6c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5184b352-78bc-4433-b128-62384a0cfef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0d313-cbce-4502-bc71-9342268b1594",
   "metadata": {},
   "source": [
    "# Here fill in the blanks with the things you want to define "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a6b21-6555-4e73-b64e-b9d97282e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "\n",
    "signs = ['', '', ''] ###################################################################################replace with your labels\n",
    "actions = np.array(['', '', '']) ###################################################################### replace with your labels\n",
    "\n",
    "\n",
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR-CONVERSION BGR-to-RGB\n",
    "    image.flags.writeable = False                  # Convert image to not-writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Convert image to writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR-COVERSION RGB-to-BGR\n",
    "    return image, results\n",
    "\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    \n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "    \n",
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "model = tf.keras.models.load_model('model.h5') ###################################################### replace with what you saved the model as \n",
    "model.load_weights('model_weights.h5') ########################################################### replace with what you saved the model weights as \n",
    "model.summary()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        #print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            #print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            #image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        #print(sentence)\n",
    "        #word = sentence[-1:]\n",
    "        #convert_to_audio(word[0]) if (len(word) != 0) else print(\"word not detected yet\")\n",
    "        #convert_to_audio(word[0])\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('Realtime LSTM Sign Language Detection', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
